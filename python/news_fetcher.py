"""
ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ëª¨ë“ˆ (Google News RSS ì‚¬ìš©)
- ë¹„íŠ¸ì½”ì¸, ë¯¸êµ­ ì¦ì‹œ, í•œêµ­ ì¦ì‹œ ë‰´ìŠ¤
- ì˜ì–´ ë‰´ìŠ¤ í•œêµ­ì–´ ë²ˆì—­ ì§€ì›
"""
import requests
import logging
import re
import xml.etree.ElementTree as ET
from typing import List, Dict, Optional
from datetime import datetime

# googletransëŠ” ì„ íƒì  (í•œêµ­ì–´ ë‰´ìŠ¤ë§Œ ì‚¬ìš©ì‹œ ë¶ˆí•„ìš”)
try:
    from googletrans import Translator
    TRANSLATOR_AVAILABLE = True
except ImportError:
    TRANSLATOR_AVAILABLE = False

logger = logging.getLogger(__name__)


class NewsFetcher:
    """Google News RSSì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” í´ë˜ìŠ¤"""

    # ë‰´ìŠ¤ í”¼ë“œ ì„¤ì • (ê²½ì œ ì „ë¬¸ ìœ„ì£¼)
    NEWS_FEEDS = {
        'mk_stock': {
            'title': 'ë§¤ê²½ ì¦ê¶Œ',
            'url_ko': 'https://www.mk.co.kr/rss/50200011/',
            'url_en': None,
            'limit': 3,
        },
        'mk_economy': {
            'title': 'ë§¤ê²½ ê²½ì œ',
            'url_ko': 'https://www.mk.co.kr/rss/30100041/',
            'url_en': None,
            'limit': 2,
        },
        'yonhap': {
            'title': 'ì—°í•©ë‰´ìŠ¤',
            'url_ko': 'https://www.yna.co.kr/rss/news.xml',
            'url_en': None,
            'limit': 2,
        },
        'kr_stock': {
            'title': 'êµ­ë‚´ ì¦ì‹œ',
            'url_ko': 'https://news.google.com/rss/search?q=êµ­ë‚´+ì¦ì‹œ&hl=ko-KR&gl=KR&ceid=KR:ko',
            'url_en': None,
            'limit': 3,
        },
        'us_stock': {
            'title': 'ë¯¸êµ­ ì¦ì‹œ',
            'url_ko': 'https://news.google.com/rss/search?q=ë¯¸êµ­+ì¦ì‹œ&hl=ko-KR&gl=KR&ceid=KR:ko',
            'url_en': None,
            'limit': 3,
        },
        'bitcoin': {
            'title': 'ë¹„íŠ¸ì½”ì¸',
            'url_ko': 'https://news.google.com/rss/search?q=ë¹„íŠ¸ì½”ì¸&hl=ko-KR&gl=KR&ceid=KR:ko',
            'url_en': None,
            'limit': 3,
        },
    }

    def __init__(self, api_key: str = None, category: str = "business"):
        self.api_key = api_key  # ë²ˆì—­ APIìš© (ì„ íƒì )
        self.category = category
        self.last_fetched_articles = []
        self._headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.translator = Translator() if TRANSLATOR_AVAILABLE else None

    def fetch_google_news_rss(self, url: str, limit: int = 10) -> List[Dict]:
        """
        Google News RSSì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°

        Args:
            url: RSS URL
            limit: ê°€ì ¸ì˜¬ ë‰´ìŠ¤ ê°œìˆ˜

        Returns:
            ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        try:
            response = requests.get(url, headers=self._headers, timeout=10)
            response.raise_for_status()

            # XML íŒŒì‹±
            root = ET.fromstring(response.content)

            articles = []
            for item in root.findall('.//item'):
                if len(articles) >= limit:
                    break

                title = item.find('title')
                link = item.find('link')
                pub_date = item.find('pubDate')
                source = item.find('source')

                article = {
                    'title': title.text if title is not None else '',
                    'link': link.text if link is not None else '',
                    'pubDate': pub_date.text if pub_date is not None else '',
                    'source': source.text if source is not None else '',
                }

                # ì œëª©ì´ ìˆëŠ” ê²ƒë§Œ ì¶”ê°€
                if article['title']:
                    articles.append(article)

            logger.info(f"Google News RSSì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ê°€ì ¸ì˜´")
            return articles

        except requests.RequestException as e:
            logger.error(f"RSS ìš”ì²­ ì˜¤ë¥˜: {e}")
            return []
        except ET.ParseError as e:
            logger.error(f"XML íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return []

    def translate_to_korean(self, text: str) -> str:
        """
        ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­ (google-translate-new ì‚¬ìš©)

        Args:
            text: ë²ˆì—­í•  í…ìŠ¤íŠ¸

        Returns:
            ë²ˆì—­ëœ í…ìŠ¤íŠ¸
        """
        if not text:
            return text

        # ì´ë¯¸ í•œêµ­ì–´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
        if self._is_korean(text):
            return text

        # ë²ˆì—­ê¸° ì—†ìœ¼ë©´ ì›ë¬¸ ë°˜í™˜
        if not self.translator:
            return text

        try:
            # googletrans ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
            translated = self.translator.translate(text, src='en', dest='ko')
            if translated and getattr(translated, 'text', None):
                return translated.text
            return text  # ë²ˆì—­ ì‹¤íŒ¨ì‹œ ì›ë¬¸ ë°˜í™˜

        except Exception as e:
            logger.debug(f"ë²ˆì—­ ì˜¤ë¥˜: {e}")
            return text

    def _is_korean(self, text: str) -> bool:
        """í•œêµ­ì–´ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
        korean_pattern = re.compile('[ê°€-í£]')
        return bool(korean_pattern.search(text))

    def _get_similarity(self, str1: str, str2: str) -> float:
        """ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard Similarity)"""
        def clean(s: str) -> set:
            s = re.sub(r'[^\w\sê°€-í£]', '', s).lower()
            return set(s.split())

        s1 = clean(str1)
        s2 = clean(str2)

        if not s1 or not s2:
            return 0.0

        intersection = s1 & s2
        union = s1 | s2

        return len(intersection) / len(union) if union else 0.0

    def _deduplicate_articles(self, articles: List[Dict], threshold: float = 0.3) -> List[Dict]:
        """ì¤‘ë³µ ê¸°ì‚¬ ì œê±°"""
        unique_articles = []

        for article in articles:
            title = article.get('title', '')
            is_duplicate = False

            for existing in unique_articles:
                existing_title = existing.get('title', '')
                similarity = self._get_similarity(title, existing_title)

                # ìœ ì‚¬ë„ 40% ì´ìƒì´ê±°ë‚˜ ì„œë¡œ í¬í•¨ê´€ê³„ë©´ ì¤‘ë³µ
                if similarity > threshold or title in existing_title or existing_title in title:
                    is_duplicate = True
                    break

            if not is_duplicate:
                unique_articles.append(article)

        return unique_articles

    def _is_duplicate_global(self, title: str, seen_titles: List[str], threshold: float = 0.3) -> bool:
        """ì „ì²´ ì¹´í…Œê³ ë¦¬ì—ì„œ ì¤‘ë³µì¸ì§€ í™•ì¸"""
        for seen in seen_titles:
            similarity = self._get_similarity(title, seen)
            if similarity > threshold or title in seen or seen in title:
                return True
        return False

    def fetch_all_news(self, translate: bool = True) -> Dict[str, List[Dict]]:
        """
        ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì „ì²´ ì¹´í…Œê³ ë¦¬ ì¤‘ë³µ ì œê±°)

        Args:
            translate: ì˜ì–´ ë‰´ìŠ¤ ë²ˆì—­ ì—¬ë¶€

        Returns:
            ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ë”•ì…”ë„ˆë¦¬
        """
        all_news = {}
        seen_titles = []  # ì „ì²´ ì¹´í…Œê³ ë¦¬ì—ì„œ ë³¸ ì œëª©ë“¤

        for category, config in self.NEWS_FEEDS.items():
            # ì¹´í…Œê³ ë¦¬ë³„ limit ì‚¬ìš© (ê¸°ë³¸ê°’ 3)
            cat_limit = config.get('limit', 3)
            articles = []

            # í•œêµ­ì–´ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            if config.get('url_ko'):
                ko_articles = self.fetch_google_news_rss(config['url_ko'], limit=cat_limit * 3)
                articles.extend(ko_articles)

            # ì˜ì–´ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ìˆëŠ” ê²½ìš°)
            if config.get('url_en'):
                en_articles = self.fetch_google_news_rss(config['url_en'], limit=cat_limit)

                # ë²ˆì—­
                if translate:
                    for article in en_articles:
                        original_title = article.get('title', '')
                        translated_title = self.translate_to_korean(original_title)

                        # ë²ˆì—­ëœ ê²½ìš° ì›ë¬¸ë„ ë³´ì¡´
                        if translated_title != original_title:
                            article['title'] = translated_title
                            article['original_title'] = original_title

                articles.extend(en_articles)

            # ì¹´í…Œê³ ë¦¬ ë‚´ ì¤‘ë³µ ì œê±°
            articles = self._deduplicate_articles(articles)

            # ì „ì²´ ì¹´í…Œê³ ë¦¬ì—ì„œ ì¤‘ë³µ ì œê±°
            unique_articles = []
            for article in articles:
                title = article.get('title', '')
                if title and not self._is_duplicate_global(title, seen_titles):
                    unique_articles.append(article)
                    seen_titles.append(title)

                if len(unique_articles) >= cat_limit:
                    break

            all_news[category] = unique_articles
            logger.info(f"{config['title']}: {len(unique_articles)}ê°œ ê¸°ì‚¬ (ì¤‘ë³µ {len(articles) - len(unique_articles)}ê°œ ì œê±°)")

        return all_news

    def format_briefing_message(self, all_news: Dict[str, List[Dict]]) -> str:
        """
        ë¸Œë¦¬í•‘ ë©”ì‹œì§€ í¬ë§·

        Args:
            all_news: ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ë”•ì…”ë„ˆë¦¬

        Returns:
            í¬ë§·ëœ ë©”ì‹œì§€
        """
        now = datetime.now().strftime("%Y-%m-%d %H:%M")
        message = f"ğŸ“¢ <b>ì˜¤ëŠ˜ì˜ ì‹œì¥ ë¸Œë¦¬í•‘</b>\n"
        message += f"ğŸ“… {now}\n\n"

        category_emojis = {
            'mk_stock': 'ğŸ“ˆ',
            'mk_economy': 'ğŸ’°',
            'yonhap': 'ğŸ“°',
            'kr_stock': 'ğŸ‡°ğŸ‡·',
            'us_stock': 'ğŸ‡ºğŸ‡¸',
            'bitcoin': 'ğŸª™',
        }

        for category, articles in all_news.items():
            config = self.NEWS_FEEDS.get(category, {})
            emoji = category_emojis.get(category, 'ğŸ“°')
            title = config.get('title', category)

            message += f"<b>{emoji} {title}</b>\n"

            if articles:
                for article in articles:  # ì¹´í…Œê³ ë¦¬ë³„ limitë§Œí¼ ì¶œë ¥
                    news_title = article.get('title', '')
                    link = article.get('link', '')
                    source = article.get('source', '')

                    if news_title:
                        if link:
                            message += f"â€¢ <a href=\"{link}\">{news_title}</a>"
                        else:
                            message += f"â€¢ {news_title}"
                        if source:
                            message += f" <i>({source})</i>"
                        message += "\n"
            else:
                message += "â€¢ ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"

            message += "\n"

        return message

    # ê¸°ì¡´ API í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œë“¤
    def fetch_everything(self, query: str, limit: int = 5, sort_by: str = "publishedAt") -> List[Dict]:
        """ê¸°ì¡´ API í˜¸í™˜ìš© - Google News RSSë¡œ ëŒ€ì²´"""
        # ì¿¼ë¦¬ì— ë”°ë¼ ì ì ˆí•œ URL ìƒì„±
        url = f"https://news.google.com/rss/search?q={query}&hl=ko&gl=KR&ceid=KR:ko"
        return self.fetch_google_news_rss(url, limit)

    def is_relevant_news(self, article: Dict) -> bool:
        """ê¸°ì‚¬ê°€ ê´€ì‹¬ ì£¼ì œì™€ ê´€ë ¨ìˆëŠ”ì§€ í™•ì¸"""
        keywords = [
            "kospi", "ì½”ìŠ¤í”¼", "í•œêµ­ì£¼ì‹", "korea stock",
            "nasdaq", "dow jones", "s&p 500", "ë¯¸êµ­ì£¼ì‹", "us stock",
            "bitcoin", "ë¹„íŠ¸ì½”ì¸", "ethereum", "ì´ë”ë¦¬ì›€", "crypto", "cryptocurrency"
        ]

        title = (article.get("title") or "").lower()
        content = title

        for keyword in keywords:
            if keyword in content:
                return True

        return False

    def format_article(self, article: Dict) -> str:
        """ê¸°ì‚¬ë¥¼ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ í¬ë§·"""
        title = article.get("title", "ì œëª© ì—†ìŒ")
        link = article.get("link", "")
        source = article.get("source", "ì¶œì²˜ ë¶ˆëª…")

        message = f"ğŸ“° <b>{title}</b>\n\n"
        message += f"ğŸ“Œ ì¶œì²˜: {source}\n"

        if link:
            message += f"ğŸ”— <a href='{link}'>ê¸°ì‚¬ ì½ê¸°</a>"

        return message
